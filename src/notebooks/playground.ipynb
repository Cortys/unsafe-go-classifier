{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import funcy as fy\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "\n",
    "import usgoc.preprocessing.graph.wl2 as wl2\n",
    "import usgoc.datasets.unsafe_go as dataset\n",
    "import usgoc.models.gnn as gnn\n",
    "import usgoc.evaluation.models as em\n",
    "import usgoc.utils as utils\n",
    "import usgoc.metrics.multi as mm\n",
    "import usgoc.evaluation.evaluate as ee\n",
    "import usgoc.postprocessing.explain as explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 2\n",
    "# mode = \"split_blocks\"\n",
    "mode = \"atomic_blocks\"\n",
    "limit_id = \"v127_d127_f127_p127_core\"\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_tf_fb\"  # \"datatype_flag\" important!\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_tf_fb_ob\"  # \"ob\" important!\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt\"\n",
    "limit_id = \"v0_d127_f0_p0_core\"\n",
    "limit_id = \"v127_d127_f127_p127\"\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_tf_fb_ob_ou_s\"  #\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_fb_ob_ou_s\"  #\n",
    "limit_id = dict(\n",
    "    varname=0, datatype=0, function=0, package=0,\n",
    "    type={\"block\", \"subblock\"},\n",
    "    # type={\"block\", \"subblock\", \"var\"},\n",
    "    blocktype=False, selfref=False, vartype=False,\n",
    "    # datatype_flag={\"Basic\", \"Interface\", \"Named\", \"Pointer\"},\n",
    "    # datatype_flag=True,\n",
    "    datatype_flag=False,\n",
    "    builtin_function=False,\n",
    "    binary_op=False, unary_op=False,\n",
    "    only_core_packages=False\n",
    ")\n",
    "limit_name = dataset.get_limit_name(limit_id)\n",
    "limit_id = \"v127_d127_f127_p127\"\n",
    "batch_size_limit = 200\n",
    "\n",
    "limit_name\n",
    "\n",
    "with utils.cache_env(use_cache=True):\n",
    "  files = dataset.load_filenames()\n",
    "  raw = dataset.load_raw()\n",
    "  ds = dataset.load_dataset(mode=mode)\n",
    "  graphs, targets = ds\n",
    "  full_dims = dataset.create_graph_dims(graphs, mode=mode)\n",
    "\n",
    "# files[683]\n",
    "# files.index(\"/app/raw/unsafe-go-dataset/app/efficiency__cast-pointer/efb20d96d7e6d3b08653.json\")\n",
    "# with utils.cache_env(use_cache=True): gs = dataset.raw_to_graphs(raw, mode=mode)\n",
    "# g = gs[683]; print(g.source_code)\n",
    "# utils.draw_graph(g, layout=\"dot\")\n",
    "# [k for k in g.types_to_pkgs.keys()]\n",
    "# g.types_to_pkgs\n",
    "# h = dataset.collect_node_label_histogram(gs, mode=mode)\n",
    "# [k for k, v in h[\"core_datatype\"].items() if not v]\n",
    "\n",
    "# graphs[0].nodes[4]\n",
    "# utils.draw_graph(graphs[0], layout=\"dot\")\n",
    "\n",
    "# len(dataset.get_dim_limit_dict().keys())\n",
    "\n",
    "# dataset.get_dim_limit_dict()[limit_id]\n",
    "\n",
    "# -%%\n",
    "\n",
    "with utils.cache_env(use_cache=True):\n",
    "  splits = dataset.get_split_idxs(ds)\n",
    "  labels1, labels2 = dataset.create_target_label_dims(ds)\n",
    "  labels1_keys = labels1.keys()\n",
    "  labels2_keys = labels2.keys()\n",
    "  labels1_inv = fy.flip(labels1)\n",
    "  labels2_inv = fy.flip(labels2)\n",
    "\n",
    "# model = gnn.MLP\n",
    "model = gnn.DeepSets\n",
    "# model = gnn.GCN\n",
    "# model = gnn.GIN\n",
    "# model = gnn.GGNN\n",
    "# model = gnn.RGCN\n",
    "# model = gnn.WL2GNN\n",
    "\n",
    "# model1 = em.DeepSetsBuilder\n",
    "# model = em.GGNNBuilder\n",
    "\n",
    "# with utils.cache_env(use_cache=True):\n",
    "#   dims, train_ds, val_ds, test_ds = dataset.get_encoded_dataset_slices(\n",
    "#       ds, model.in_enc, splits, fold, limit_id=limit_id, mode=mode,\n",
    "#       batch_size_limit=batch_size_limit)\n",
    "#   train_ds = train_ds.cache()\n",
    "#   val_ds = val_ds.cache()\n",
    "#   train_slice, val_slice, test_slice = dataset.get_dataset_slices(\n",
    "#       ds, splits, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_str():\n",
    "  return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "def experiment(model, epochs=150, log=True):\n",
    "  if isinstance(model, kt.HyperModel):\n",
    "    tuner = kt.Hyperband(\n",
    "        model,\n",
    "        objective=\"val_accuracy\",\n",
    "        max_epochs=200, factor=3,\n",
    "        hyperband_iterations=1,\n",
    "        directory=f\"{utils.PROJECT_ROOT}/evaluations\",\n",
    "        project_name=f\"playground_{model.name}\")\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=30)\n",
    "    tuner.search(\n",
    "        train_ds, validation_data=val_ds, verbose=2, epochs=500,\n",
    "        callbacks=[stop_early])\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(\"Best HPs:\", best_hps)\n",
    "    m = tuner.hypermodel.build(best_hps)\n",
    "    patient_stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=100, restore_best_weights=True)\n",
    "    m.fit(\n",
    "        train_ds, validation_data=val_ds, verbose=2, epochs=1000,\n",
    "        callbacks=[patient_stop_early])\n",
    "    res = m.evaluate(test_ds, return_dict=True)\n",
    "    print(res)\n",
    "    return m\n",
    "  else:\n",
    "    m = model(\n",
    "        node_label_count=dims[\"node_label_count\"],\n",
    "        conv_directed=True,\n",
    "        # conv_reverse=False,\n",
    "        conv_layer_units=[400] * 4, fc_layer_units=[200] * 2,\n",
    "        conv_activation=\"tanh\",\n",
    "        conv_inner_activation=\"elu\",\n",
    "        fc_activation=\"tanh\",\n",
    "        pooling=\"min\",\n",
    "        # For 2-WL-GNN:\n",
    "        # conv_activation=\"tanh\",\n",
    "        # conv_inner_activation=\"tanh\",\n",
    "        # fc_activation=\"elu\",\n",
    "        # pooling=\"mean\",\n",
    "        #\n",
    "        # conv_dropout_rate=0.1,\n",
    "        # fc_dropout_rate=0.2,\n",
    "        conv_batch_norm=True,\n",
    "        fc_batch_norm=True,\n",
    "        out_activation=None,\n",
    "        learning_rate=0.001)\n",
    "\n",
    "    tb = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=f\"/app/logs/{time_str()}_{model.name}_{limit_name}_fold{fold}\",\n",
    "        histogram_freq=10,\n",
    "        embeddings_freq=10,\n",
    "        write_graph=True,\n",
    "        update_freq=\"batch\")\n",
    "    patient_stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=100, restore_best_weights=True)\n",
    "    m.fit(\n",
    "        train_ds, validation_data=val_ds, verbose=2, epochs=epochs,\n",
    "        callbacks=[tb, patient_stop_early] if log else [patient_stop_early])\n",
    "    res = m.evaluate(test_ds, return_dict=True)\n",
    "    print(res)\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening experiment DeepSets...\n",
      "Starting outer usgo_v1/atomic_blocks/v127_d127_f127_p127 for DeepSets...\n",
      "Skipping usgo_v1/atomic_blocks_v127_d127_f127_p127_fold0_repeat0, DeepSets. Existing run: 918d380483654ec8b9ef3e70c3f634a5.\n",
      "Skipping usgo_v1/atomic_blocks_v127_d127_f127_p127_fold1_repeat0, DeepSets. Existing run: 2444e2b830204fb4832a424c8132000c.\n",
      "Completed outer usgo_v1/v127_d127_f127_p127 for DeepSets.\n",
      "Closing experiment DeepSets.\n"
     ]
    }
   ],
   "source": [
    "# mm._groups = dict()\n",
    "# m = experiment(model1)\n",
    "# m.save(f\"{utils.PROJECT_ROOT}/logs/test\")\n",
    "# m2 = tf.keras.models.load_model(f\"{utils.PROJECT_ROOT}/logs/test\", custom_objects=dict(SparseMultiAccuracy=mm.SparseMultiAccuracy))\n",
    "\n",
    "(m0, cc0, dims0, train0, val0, test0), (m1, cc1, dims1, train1, val1, test1) = ee.evaluate(model.name, convert_mode=mode,\n",
    "  limit_id=limit_id, folds=2, repeat=0, dry=True,\n",
    "  return_models=True, return_calibration_configs=True, return_ds=True)\n",
    "# m[0][2].evaluate(test_ds, return_dict=True)\n",
    "# m = experiment(model1)\n",
    "\n",
    "# m2 = experiment(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.6923077 , 0.3653846 , 0.63461536, 0.5576923 ,\n",
       "        0.25      , 0.32692307, 0.59615386, 0.46153846, 0.13461539,\n",
       "        0.1923077 ],\n",
       "       [0.85714287, 1.        , 0.42857143, 0.64285713, 0.7380952 ,\n",
       "        0.23809524, 0.23809524, 0.54761904, 0.35714287, 0.14285715,\n",
       "        0.1904762 ],\n",
       "       [0.7307692 , 0.6923077 , 1.        , 0.6923077 , 0.8076923 ,\n",
       "        0.30769232, 0.23076923, 0.5       , 0.5       , 0.1923077 ,\n",
       "        0.15384616],\n",
       "       [0.8918919 , 0.7297297 , 0.4864865 , 1.        , 0.5945946 ,\n",
       "        0.3783784 , 0.3783784 , 0.7027027 , 0.5405405 , 0.16216215,\n",
       "        0.13513513],\n",
       "       [0.3580247 , 0.38271606, 0.25925925, 0.27160493, 1.        ,\n",
       "        0.14814815, 0.17283951, 0.24691358, 0.19753087, 0.08641975,\n",
       "        0.12345679],\n",
       "       [0.5652174 , 0.4347826 , 0.3478261 , 0.6086956 , 0.5217391 ,\n",
       "        1.        , 0.4347826 , 0.6086956 , 0.47826087, 0.04347826,\n",
       "        0.17391305],\n",
       "       [0.53125   , 0.3125    , 0.1875    , 0.4375    , 0.4375    ,\n",
       "        0.3125    , 1.        , 0.53125   , 0.59375   , 0.125     ,\n",
       "        0.34375   ],\n",
       "       [0.8378378 , 0.6216216 , 0.35135135, 0.7027027 , 0.5405405 ,\n",
       "        0.3783784 , 0.45945945, 1.        , 0.6216216 , 0.13513513,\n",
       "        0.35135135],\n",
       "       [0.7741935 , 0.48387095, 0.41935483, 0.6451613 , 0.516129  ,\n",
       "        0.3548387 , 0.61290324, 0.7419355 , 1.        , 0.19354838,\n",
       "        0.32258064],\n",
       "       [0.3181818 , 0.27272728, 0.22727273, 0.27272728, 0.3181818 ,\n",
       "        0.04545455, 0.18181819, 0.22727273, 0.27272728, 1.        ,\n",
       "        0.09090909],\n",
       "       [0.7692308 , 0.61538464, 0.30769232, 0.3846154 , 0.7692308 ,\n",
       "        0.30769232, 0.84615386, 1.        , 0.7692308 , 0.15384616,\n",
       "        1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import usgoc.utils as utils\n",
    "import usgoc.postprocessing.conformal as conformal\n",
    "import usgoc.metrics.correlation as corr\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "def predict_conformal_sets(m, ds, config):\n",
    "  pred1, pred2 = m.predict(ds)\n",
    "  pred1 /= config[\"t1\"]\n",
    "  pred2 /= config[\"t2\"]\n",
    "  sets1 = conformal.adaptive_sets(pred1, config[\"qhat1\"])\n",
    "  sets2 = conformal.adaptive_sets(pred2, config[\"qhat2\"])\n",
    "  return sets1, sets2\n",
    "\n",
    "sets1, sets2 = predict_conformal_sets(m0, test0, cc0[alpha])\n",
    "\n",
    "utils.row_normalize_matrix(corr.sets_to_cooccurrence(sets1), False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_graph(i=None, file=None, test_j=None, val_j=None, draw=True):\n",
    "  if i is None:\n",
    "    if file is not None:\n",
    "      i = fy.first(fy.filter(\n",
    "          lambda e: file in e[1], enumerate(files)[0]))\n",
    "    elif test_j is not None:\n",
    "      i = splits[fold][\"test\"][test_j]\n",
    "    elif val_j is not None:\n",
    "      i = splits[fold][\"validation\"][val_j]\n",
    "  print(graphs[i].source_code)\n",
    "  print(files[i], labels1_inv[targets[0][i]], labels2_inv[targets[1][i]])\n",
    "  if draw:\n",
    "    utils.draw_graph(graphs[i], edge_colors=True, layout=\"dot\")\n",
    "  return i\n",
    "\n",
    "\n",
    "def get_singleton_ds(i):\n",
    "  with utils.cache_env(use_cache=False):\n",
    "    return dataset.dataset_encoders[model.in_enc](dataset.slice(ds, [i]), dims)\n",
    "\n",
    "\n",
    "def preds_to_dicts(preds, j):\n",
    "  prob1 = np.around(tf.nn.softmax(preds[0], -1).numpy()[j], 3)\n",
    "  prob2 = np.around(tf.nn.softmax(preds[1], -1).numpy()[j], 3)\n",
    "  d1 = fy.zipdict(labels1_keys, zip(list(prob1), list(preds[0][j])))\n",
    "  d2 = fy.zipdict(labels2_keys, zip(list(prob2), list(preds[1][j])))\n",
    "  return d1, d2\n",
    "\n",
    "\n",
    "def draw_confusion(pred_labels, target_labels, normalize=True):\n",
    "  m1 = mm.sparse_multi_confusion_matrix(\n",
    "      tf.constant(target_labels[0], dtype=tf.int32),\n",
    "      tf.constant(pred_labels[0], dtype=tf.int32)).numpy()\n",
    "  m2 = mm.sparse_multi_confusion_matrix(\n",
    "      tf.constant(target_labels[1], dtype=tf.int32),\n",
    "      tf.constant(pred_labels[1], dtype=tf.int32)).numpy()\n",
    "  a1 = np.sum(np.diag(m1)) / np.sum(m1)\n",
    "  a2 = np.sum(np.diag(m2)) / np.sum(m2)\n",
    "  print(\"L1-Acc:\", a1, \"L2-Acc:\", a2)\n",
    "  if normalize:\n",
    "    m1 = np.around(m1 / np.sum(m1, axis=1, keepdims=True), 2) * 100\n",
    "    m2 = np.around(m2 / np.sum(m2, axis=1, keepdims=True), 2) * 100\n",
    "  utils.draw_confusion_matrix(m1.astype(int), labels1_keys)\n",
    "  utils.draw_confusion_matrix(m2.astype(int), labels2_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_graph(file=\"5179906774f18e1f8520\", draw=False)\n",
    "# debug_graph(401)\n",
    "\n",
    "\n",
    "# interesting i's: 40, 70, 874 (44b41ab329d2624a449e),\n",
    "# Instances on which both DeepSets and GGNN fail (both labels):\n",
    "# - 43 (a793d8b779605120ec52) true: cast-basic ffi (short snippet)\n",
    "# - 79 (836d21cfe55279cd8e46) true: delegate atomic (long snippet)\n",
    "# - 1236 (27b69b7c042b73cf9275) true: memory-access layout (short/medium snippet)\n",
    "\n",
    "# Instances on wihich only GGNN fails (but not DeepSets):\n",
    "# - 84 (b01a05c45ecc95b792c8) true: cast-header types (short/medium)\n",
    "#   Code:\n",
    "#   func bytesHash(b []byte, seed uintptr) uintptr {\n",
    "#\t    s := (*slice)(unsafe.Pointer(&b))\n",
    "#\t    return memhash(s.array, seed, uintptr(s.len))\n",
    "#   }\n",
    "# - 1015 (39edbcf5d9443fc73e3a) true: pointer-arithmetic serialization (short)\n",
    "# - 1313 (b54f66ea4c799f26617d) true: cast-struct generics (short)\n",
    "#   Code: func (h *Header) Int8s() []int8 { return *(*[]int8)(unsafe.Pointer(h)) }\n",
    "#   DeepSets Preds:\n",
    "#   ({'cast-basic': (0.544, 13.852097),\n",
    "#     'cast-bytes': (0.0, -0.26986185),\n",
    "#     'cast-header': (0.0, -3.9276118),\n",
    "#     'cast-pointer': (0.455, 13.672801),\n",
    "#     'cast-struct': (0.001, 7.118796),\n",
    "#     'definition': (0.0, -37.71391),\n",
    "#     'delegate': (0.0, -3.9902112),\n",
    "#     'memory-access': (0.0, -4.7687745),\n",
    "#     'pointer-arithmetic': (0.0, -17.577158),\n",
    "#     'syscall': (0.0, -22.886244),\n",
    "#     'unused': (0.0, -5.127344)},\n",
    "#    {'atomic': (0.0, -8.450554),\n",
    "#     'efficiency': (0.001, 3.8338575),\n",
    "#     'ffi': (0.0, 0.4648861),\n",
    "#     'generics': (0.994, 10.651982),\n",
    "#     'hide-escape': (0.0, -19.393679),\n",
    "#     'layout': (0.0, -3.1287308),\n",
    "#     'no-gc': (0.0, -17.564428),\n",
    "#     'reflect': (0.0, 1.974027),\n",
    "#     'serialization': (0.0, 0.13683213),\n",
    "#     'types': (0.004, 5.250824),\n",
    "#     'unused': (0.0, -0.42025906)})\n",
    "#   GGNN Preds:\n",
    "#   ({'cast-basic': (0.0, 1.3986396),\n",
    "#     'cast-bytes': (0.993, 19.658085),\n",
    "#     'cast-header': (0.0, -14.408668),\n",
    "#     'cast-pointer': (0.0, 11.910549),\n",
    "#     'cast-struct': (0.0, 6.641031),\n",
    "#     'definition': (0.0, -29.842493),\n",
    "#     'delegate': (0.006, 14.610508),\n",
    "#     'memory-access': (0.0, 5.646147),\n",
    "#     'pointer-arithmetic': (0.0, -32.213),\n",
    "#     'syscall': (0.0, -41.637238),\n",
    "#     'unused': (0.0, -21.783506)},\n",
    "#    {'atomic': (0.0, 10.781407),\n",
    "#     'efficiency': (0.007, 13.999677),\n",
    "#     'ffi': (0.0, -11.787495),\n",
    "#     'generics': (0.0, 11.160701),\n",
    "#     'hide-escape': (0.0, -27.821772),\n",
    "#     'layout': (0.0, -1.9805977),\n",
    "#     'no-gc': (0.0, -40.828663),\n",
    "#     'reflect': (0.0, 3.5758069),\n",
    "#     'serialization': (0.993, 18.984678),\n",
    "#     'types': (0.0, -6.821523),\n",
    "#     'unused': (0.0, -30.649702)})\n",
    "# - 1393 (a4c0182265f30feaa1f2) true: cast-bytes efficiency\n",
    "#   Code: func (m *SortedMap) Get(key string) (value interface{}, ok bool) {\n",
    "#\t          return m.trie.Get(*(*[]byte)(unsafe.Pointer(&key)))\n",
    "#         }\n",
    "\n",
    "# NOTE: On first, third and last two examples serialization was wrongly predicted by GNN (instead of generics/efficiency). Why? DeepSet correctly detects generic, but not efficiency\n",
    "\n",
    "# train_pred = m.predict(train_ds)\n",
    "# train_pred2 = m2.predict(train_ds)\n",
    "# s_pred = m.predict(test_ds)\n",
    "# s_pred2 = m2.predict(test_ds)\n",
    "# s_pred_labels = tf.cast(tf.stack([tf.argmax(s_pred[0], -1), tf.argmax(s_pred[1], -1)], 1), tf.int32)\n",
    "# s_pred_labels2 = tf.cast(tf.stack([tf.argmax(s_pred2[0], -1), tf.argmax(s_pred2[1], -1)], 1), tf.int32)\n",
    "# target_labels = tf.stack(list(test_ds)[0][1], 1)\n",
    "# s_pred\n",
    "# draw_confusion(train_pred, train_slice[1], True)\n",
    "# draw_confusion(train_pred2, train_slice[1], True)\n",
    "# draw_confusion(s_pred, test_slice[1], True)\n",
    "# draw_confusion(s_pred2, test_slice[1], True)\n",
    "#\n",
    "# pred_matches = (s_pred_labels == target_labels).numpy()\n",
    "# pred_matches2 = (s_pred_labels2 == target_labels).numpy()\n",
    "# problem_js = np.where(~(pred_matches[:, 0] | pred_matches[:, 1]))[0]\n",
    "# problem_js2 = np.where(~(pred_matches2[:, 0] | pred_matches2[:, 1]))[0]\n",
    "# set(problem_js) & set(problem_js2)\n",
    "# set(problem_js2) - set(problem_js)\n",
    "# len(problem_js2)\n",
    "#\n",
    "# # j = problem_js[4]\n",
    "# j = 32; i = None\n",
    "# # i = 84\n",
    "# i = debug_graph(i=i, test_j=j, draw=True)\n",
    "# i\n",
    "# preds_to_dicts(s_pred, j)\n",
    "# preds_to_dicts(s_pred2, j)\n",
    "# s_pred_labels2[j]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
