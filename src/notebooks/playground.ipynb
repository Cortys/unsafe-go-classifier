{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import funcy as fy\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "\n",
    "import usgoc.preprocessing.graph.wl2 as wl2\n",
    "import usgoc.datasets.unsafe_go as dataset\n",
    "import usgoc.models.gnn as gnn\n",
    "import usgoc.evaluation.models as em\n",
    "import usgoc.utils as utils\n",
    "import usgoc.metrics.multi as mm\n",
    "import usgoc.evaluation.evaluate as ee\n",
    "import usgoc.postprocessing.explain as explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 2\n",
    "# mode = \"split_blocks\"\n",
    "mode = \"atomic_blocks\"\n",
    "limit_id = \"v127_d127_f127_p127_core\"\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_tf_fb\"  # \"datatype_flag\" important!\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_tf_fb_ob\"  # \"ob\" important!\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt\"\n",
    "limit_id = \"v0_d127_f0_p0_core\"\n",
    "limit_id = \"v127_d127_f127_p127\"\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_tf_fb_ob_ou_s\"  #\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_fb_ob_ou_s\"  #\n",
    "limit_id = dict(\n",
    "    varname=0, datatype=0, function=0, package=0,\n",
    "    type={\"block\", \"subblock\"},\n",
    "    # type={\"block\", \"subblock\", \"var\"},\n",
    "    blocktype=False, selfref=False, vartype=False,\n",
    "    # datatype_flag={\"Basic\", \"Interface\", \"Named\", \"Pointer\"},\n",
    "    # datatype_flag=True,\n",
    "    datatype_flag=False,\n",
    "    builtin_function=False,\n",
    "    binary_op=False, unary_op=False,\n",
    "    only_core_packages=False\n",
    ")\n",
    "limit_name = dataset.get_limit_name(limit_id)\n",
    "limit_id = \"v127_d127_f127_p127\"\n",
    "batch_size_limit = 200\n",
    "\n",
    "limit_name\n",
    "\n",
    "with utils.cache_env(use_cache=True):\n",
    "  files = dataset.load_filenames()\n",
    "  raw = dataset.load_raw()\n",
    "  ds = dataset.load_dataset(mode=mode)\n",
    "  graphs, targets = ds\n",
    "  full_dims = dataset.create_graph_dims(graphs, mode=mode)\n",
    "\n",
    "# files[683]\n",
    "# files.index(\"/app/raw/unsafe-go-dataset/app/efficiency__cast-pointer/efb20d96d7e6d3b08653.json\")\n",
    "# with utils.cache_env(use_cache=True): gs = dataset.raw_to_graphs(raw, mode=mode)\n",
    "# g = gs[683]; print(g.source_code)\n",
    "# utils.draw_graph(g, layout=\"dot\")\n",
    "# [k for k in g.types_to_pkgs.keys()]\n",
    "# g.types_to_pkgs\n",
    "# h = dataset.collect_node_label_histogram(gs, mode=mode)\n",
    "# [k for k, v in h[\"core_datatype\"].items() if not v]\n",
    "\n",
    "# graphs[0].nodes[4]\n",
    "# utils.draw_graph(graphs[0], layout=\"dot\")\n",
    "\n",
    "# len(dataset.get_dim_limit_dict().keys())\n",
    "\n",
    "# dataset.get_dim_limit_dict()[limit_id]\n",
    "\n",
    "# -%%\n",
    "\n",
    "with utils.cache_env(use_cache=True):\n",
    "  splits = dataset.get_split_idxs(ds)\n",
    "  labels1, labels2 = dataset.create_target_label_dims(ds)\n",
    "  labels1_keys = labels1.keys()\n",
    "  labels2_keys = labels2.keys()\n",
    "  labels1_inv = fy.flip(labels1)\n",
    "  labels2_inv = fy.flip(labels2)\n",
    "\n",
    "# model = gnn.MLP\n",
    "# model = gnn.DeepSets\n",
    "# model = gnn.GCN\n",
    "# model = gnn.GIN\n",
    "# model = gnn.GGNN\n",
    "# model = gnn.RGCN\n",
    "model = gnn.WL2GNN\n",
    "\n",
    "# model1 = em.DeepSetsBuilder\n",
    "# model = em.GGNNBuilder\n",
    "\n",
    "# with utils.cache_env(use_cache=True):\n",
    "#   dims, train_ds, val_ds, test_ds = dataset.get_encoded_dataset_slices(\n",
    "#       ds, model.in_enc, splits, fold, limit_id=limit_id, mode=mode,\n",
    "#       batch_size_limit=batch_size_limit)\n",
    "#   train_ds = train_ds.cache()\n",
    "#   val_ds = val_ds.cache()\n",
    "#   train_slice, val_slice, test_slice = dataset.get_dataset_slices(\n",
    "#       ds, splits, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dots = [nx.nx_agraph.to_agraph(g).to_string() for g in graphs]\n",
    "with open(\"/app/results/graphs_dot.json\", \"w\") as f:\n",
    "  json.dump(dots, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_str():\n",
    "  return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "def experiment(model, epochs=150, log=True):\n",
    "  if isinstance(model, kt.HyperModel):\n",
    "    tuner = kt.Hyperband(\n",
    "        model,\n",
    "        objective=\"val_accuracy\",\n",
    "        max_epochs=200, factor=3,\n",
    "        hyperband_iterations=1,\n",
    "        directory=f\"{utils.PROJECT_ROOT}/evaluations\",\n",
    "        project_name=f\"playground_{model.name}\")\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=30)\n",
    "    tuner.search(\n",
    "        train_ds, validation_data=val_ds, verbose=2, epochs=500,\n",
    "        callbacks=[stop_early])\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(\"Best HPs:\", best_hps)\n",
    "    m = tuner.hypermodel.build(best_hps)\n",
    "    patient_stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=100, restore_best_weights=True)\n",
    "    m.fit(\n",
    "        train_ds, validation_data=val_ds, verbose=2, epochs=1000,\n",
    "        callbacks=[patient_stop_early])\n",
    "    res = m.evaluate(test_ds, return_dict=True)\n",
    "    print(res)\n",
    "    return m\n",
    "  else:\n",
    "    m = model(\n",
    "        node_label_count=dims[\"node_label_count\"],\n",
    "        conv_directed=True,\n",
    "        # conv_reverse=False,\n",
    "        conv_layer_units=[400] * 4, fc_layer_units=[200] * 2,\n",
    "        conv_activation=\"tanh\",\n",
    "        conv_inner_activation=\"elu\",\n",
    "        fc_activation=\"tanh\",\n",
    "        pooling=\"min\",\n",
    "        # For 2-WL-GNN:\n",
    "        # conv_activation=\"tanh\",\n",
    "        # conv_inner_activation=\"tanh\",\n",
    "        # fc_activation=\"elu\",\n",
    "        # pooling=\"mean\",\n",
    "        #\n",
    "        # conv_dropout_rate=0.1,\n",
    "        # fc_dropout_rate=0.2,\n",
    "        conv_batch_norm=True,\n",
    "        fc_batch_norm=True,\n",
    "        out_activation=None,\n",
    "        learning_rate=0.001)\n",
    "\n",
    "    tb = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=f\"/app/logs/{time_str()}_{model.name}_{limit_name}_fold{fold}\",\n",
    "        histogram_freq=10,\n",
    "        embeddings_freq=10,\n",
    "        write_graph=True,\n",
    "        update_freq=\"batch\")\n",
    "    patient_stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=100, restore_best_weights=True)\n",
    "    m.fit(\n",
    "        train_ds, validation_data=val_ds, verbose=2, epochs=epochs,\n",
    "        callbacks=[tb, patient_stop_early] if log else [patient_stop_early])\n",
    "    res = m.evaluate(test_ds, return_dict=True)\n",
    "    print(res)\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening experiment WL2GNN...\n",
      "Starting outer usgo_v1/atomic_blocks/v127_d127_f127_p127 for WL2GNN...\n",
      "Skipping usgo_v1/atomic_blocks_v127_d127_f127_p127_fold0_repeat0, WL2GNN. Existing run: 3857c336318349e6a94107db726ecfe1.\n",
      "Skipping usgo_v1/atomic_blocks_v127_d127_f127_p127_fold1_repeat0, WL2GNN. Existing run: 6f208085debc47bc849a2ab17adf06ba.\n",
      "Skipping usgo_v1/atomic_blocks_v127_d127_f127_p127_fold2_repeat0, WL2GNN. Existing run: 64ca1972490646778341b633b8b0840f.\n",
      "Skipping usgo_v1/atomic_blocks_v127_d127_f127_p127_fold3_repeat0, WL2GNN. Existing run: 303894f4ff3842b5b94acb890e93fb08.\n",
      "Skipping usgo_v1/atomic_blocks_v127_d127_f127_p127_fold4_repeat0, WL2GNN. Existing run: 03c9564ac9d44bb6a1c4a5689a9e728d.\n",
      "Skipping usgo_v1/atomic_blocks_v127_d127_f127_p127_fold5_repeat0, WL2GNN. Existing run: 5d6f5c51df174759aff05d40efa6fee0.\n",
      "Skipping usgo_v1/atomic_blocks_v127_d127_f127_p127_fold6_repeat0, WL2GNN. Existing run: aef422dddbe44d15aedf57dd73edf38e.\n",
      "Skipping usgo_v1/atomic_blocks_v127_d127_f127_p127_fold7_repeat0, WL2GNN. Existing run: b63bc5199d344814a23dba0ce7ea21ee.\n",
      "Skipping usgo_v1/atomic_blocks_v127_d127_f127_p127_fold8_repeat0, WL2GNN. Existing run: 2b3290b769cf4ea2aaca7e445b0f3450.\n",
      "Skipping usgo_v1/atomic_blocks_v127_d127_f127_p127_fold9_repeat0, WL2GNN. Existing run: f080a2597e224a46b8bef26145363cff.\n",
      "Completed outer usgo_v1/v127_d127_f127_p127 for WL2GNN.\n",
      "Closing experiment WL2GNN.\n"
     ]
    }
   ],
   "source": [
    "# mm._groups = dict()\n",
    "# m = experiment(model1)\n",
    "# m.save(f\"{utils.PROJECT_ROOT}/logs/test\")\n",
    "# m2 = tf.keras.models.load_model(f\"{utils.PROJECT_ROOT}/logs/test\", custom_objects=dict(SparseMultiAccuracy=mm.SparseMultiAccuracy))\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "folds = ee.evaluate(model.name, convert_mode=mode,\n",
    "  limit_id=limit_id, repeat=0, dry=True,\n",
    "  return_models=True, return_calibration_configs=True, return_ds=True)\n",
    "# m[0][2].evaluate(test_ds, return\n",
    "# m = experiment(model1)\n",
    "\n",
    "# m2 = experiment(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import usgoc.utils as utils\n",
    "import usgoc.models.utils as mu\n",
    "import usgoc.metrics.correlation as corr\n",
    "import json\n",
    "\n",
    "labels1_keys = list(labels1.keys())\n",
    "labels2_keys = list(labels2.keys())\n",
    "alpha = 0.1\n",
    "\n",
    "idxs = []\n",
    "sets1 = []\n",
    "sets2 = []\n",
    "preds1 = []\n",
    "preds2 = []\n",
    "sizes1 = []\n",
    "sizes2 = []\n",
    "fimps1 = []\n",
    "fimps2 = []\n",
    "\n",
    "def importance_score_dict(fimps, dim_names, labels, k=10):\n",
    "  fimps_idx = np.argsort(fimps, axis=1)\n",
    "  if k > 0 and k < dim_names.shape[0] / 2:\n",
    "    fimps_idx = np.concatenate([\n",
    "      fimps_idx[:, :k],\n",
    "      fimps_idx[:, -k:]\n",
    "    ], axis=1)\n",
    "\n",
    "  res = dict()\n",
    "  for lbl, lbl_fimps, idx in zip(labels, fimps, fimps_idx):\n",
    "    lbl_dim_names_sorted = dim_names[idx][::-1]\n",
    "    lbl_fimps_sorted = lbl_fimps[idx][::-1]\n",
    "    res[lbl] = [\n",
    "      dict(feature=n, importance=imp)\n",
    "      for n, imp in zip(lbl_dim_names_sorted, lbl_fimps_sorted)]\n",
    "\n",
    "  return res\n",
    "\n",
    "def importance_score_dicts(model, ds, dims, labels1, labels2, k=3):\n",
    "  fimps1, fimps2 = explain.compute_importances(model, ds)\n",
    "  dim_names = np.array(dataset.dims_to_labels(dims, dims[\"in_enc\"]))\n",
    "  n = fimps1.shape[1]\n",
    "  ds1, ds2 = [], []\n",
    "  for i in range(n):\n",
    "    d1 = importance_score_dict(fimps1[:,i,:], dim_names, labels1, k)\n",
    "    d2 = importance_score_dict(fimps2[:,i,:], dim_names, labels2, k)\n",
    "    ds1.append(d1)\n",
    "    ds2.append(d2)\n",
    "  return ds1, ds2\n",
    "\n",
    "for (m, cc, dims, train, val, test), split in zip(folds, splits):\n",
    "  test_idxs = split[\"test\"]\n",
    "  idxs.append(test_idxs)\n",
    "  s1, s2, p1, p2 = mu.predict_conformal(m, test, with_preds=True, **cc[alpha])\n",
    "  i1, i2 = importance_score_dicts(\n",
    "    m, test, {**dims, \"in_enc\": model.in_enc}, labels1_keys, labels2_keys)\n",
    "  fimps1 += i1\n",
    "  fimps2 += i2\n",
    "  sets1 += s1\n",
    "  sets2 += s2\n",
    "  preds1.append(tf.nn.softmax(p1).numpy())\n",
    "  preds2.append(tf.nn.softmax(p2).numpy())\n",
    "  sizes1.append(np.array([len(s) for s in s1], dtype=np.int32))\n",
    "  sizes2.append(np.array([len(s) for s in s2], dtype=np.int32))\n",
    "\n",
    "sizes1 = np.concatenate(sizes1, 0)\n",
    "sizes2 = np.concatenate(sizes2, 0)\n",
    "sizes = sizes1 + sizes2\n",
    "sort_idxs = np.argsort(sizes)[::-1]\n",
    "sizes = sizes[sort_idxs]\n",
    "idxs = np.concatenate(idxs, 0)[sort_idxs]\n",
    "preds1 = np.concatenate(preds1, 0)[sort_idxs]\n",
    "preds2 = np.concatenate(preds2, 0)[sort_idxs]\n",
    "sets1 = utils.obj_array(sets1)[sort_idxs]\n",
    "sets2 = utils.obj_array(sets2)[sort_idxs]\n",
    "fimps1 = utils.obj_array(fimps1)[sort_idxs]\n",
    "fimps2 = utils.obj_array(fimps2)[sort_idxs]\n",
    "\n",
    "res = []\n",
    "\n",
    "for idx, size, s1, s2, p1, p2, i1, i2 in zip(\n",
    "  idxs, sizes, sets1, sets2, preds1, preds2, fimps1, fimps2):\n",
    "  res.append(dict(\n",
    "    idx=idx,\n",
    "    size=size,\n",
    "    conformal_sets=(\n",
    "      fy.lmap(lambda i: (labels1_keys[i], p1[i]), s1),\n",
    "      fy.lmap(lambda i: (labels2_keys[i], p2[i]), s2)),\n",
    "    code=graphs[idx].source_code,\n",
    "    feature_importance_scores=(i1, i2),\n",
    "    target=(labels1_keys[targets[0][idx]], labels2_keys[targets[1][idx]])\n",
    "  ))\n",
    "\n",
    "with open(f\"/app/results/preds_{model.name}.json\", \"w\") as f:\n",
    "  json.dump(res, f, cls=utils.NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_graph(i=None, file=None, test_j=None, val_j=None, draw=True):\n",
    "  if i is None:\n",
    "    if file is not None:\n",
    "      i = fy.first(fy.filter(\n",
    "          lambda e: file in e[1], enumerate(files)[0]))\n",
    "    elif test_j is not None:\n",
    "      i = splits[fold][\"test\"][test_j]\n",
    "    elif val_j is not None:\n",
    "      i = splits[fold][\"validation\"][val_j]\n",
    "  print(graphs[i].source_code)\n",
    "  print(files[i], labels1_inv[targets[0][i]], labels2_inv[targets[1][i]])\n",
    "  if draw:\n",
    "    utils.draw_graph(graphs[i], edge_colors=True, layout=\"dot\")\n",
    "  return i\n",
    "\n",
    "\n",
    "def get_singleton_ds(i):\n",
    "  with utils.cache_env(use_cache=False):\n",
    "    return dataset.dataset_encoders[model.in_enc](dataset.slice(ds, [i]), dims)\n",
    "\n",
    "\n",
    "def preds_to_dicts(preds, j):\n",
    "  prob1 = np.around(tf.nn.softmax(preds[0], -1).numpy()[j], 3)\n",
    "  prob2 = np.around(tf.nn.softmax(preds[1], -1).numpy()[j], 3)\n",
    "  d1 = fy.zipdict(labels1_keys, zip(list(prob1), list(preds[0][j])))\n",
    "  d2 = fy.zipdict(labels2_keys, zip(list(prob2), list(preds[1][j])))\n",
    "  return d1, d2\n",
    "\n",
    "\n",
    "def draw_confusion(pred_labels, target_labels, normalize=True):\n",
    "  m1 = mm.sparse_multi_confusion_matrix(\n",
    "      tf.constant(target_labels[0], dtype=tf.int32),\n",
    "      tf.constant(pred_labels[0], dtype=tf.int32)).numpy()\n",
    "  m2 = mm.sparse_multi_confusion_matrix(\n",
    "      tf.constant(target_labels[1], dtype=tf.int32),\n",
    "      tf.constant(pred_labels[1], dtype=tf.int32)).numpy()\n",
    "  a1 = np.sum(np.diag(m1)) / np.sum(m1)\n",
    "  a2 = np.sum(np.diag(m2)) / np.sum(m2)\n",
    "  print(\"L1-Acc:\", a1, \"L2-Acc:\", a2)\n",
    "  if normalize:\n",
    "    m1 = np.around(m1 / np.sum(m1, axis=1, keepdims=True), 2) * 100\n",
    "    m2 = np.around(m2 / np.sum(m2, axis=1, keepdims=True), 2) * 100\n",
    "  utils.draw_confusion_matrix(m1.astype(int), labels1_keys)\n",
    "  utils.draw_confusion_matrix(m2.astype(int), labels2_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_graph(file=\"5179906774f18e1f8520\", draw=False)\n",
    "# debug_graph(401)\n",
    "\n",
    "\n",
    "# interesting i's: 40, 70, 874 (44b41ab329d2624a449e),\n",
    "# Instances on which both DeepSets and GGNN fail (both labels):\n",
    "# - 43 (a793d8b779605120ec52) true: cast-basic ffi (short snippet)\n",
    "# - 79 (836d21cfe55279cd8e46) true: delegate atomic (long snippet)\n",
    "# - 1236 (27b69b7c042b73cf9275) true: memory-access layout (short/medium snippet)\n",
    "\n",
    "# Instances on wihich only GGNN fails (but not DeepSets):\n",
    "# - 84 (b01a05c45ecc95b792c8) true: cast-header types (short/medium)\n",
    "#   Code:\n",
    "#   func bytesHash(b []byte, seed uintptr) uintptr {\n",
    "#\t    s := (*slice)(unsafe.Pointer(&b))\n",
    "#\t    return memhash(s.array, seed, uintptr(s.len))\n",
    "#   }\n",
    "# - 1015 (39edbcf5d9443fc73e3a) true: pointer-arithmetic serialization (short)\n",
    "# - 1313 (b54f66ea4c799f26617d) true: cast-struct generics (short)\n",
    "#   Code: func (h *Header) Int8s() []int8 { return *(*[]int8)(unsafe.Pointer(h)) }\n",
    "#   DeepSets Preds:\n",
    "#   ({'cast-basic': (0.544, 13.852097),\n",
    "#     'cast-bytes': (0.0, -0.26986185),\n",
    "#     'cast-header': (0.0, -3.9276118),\n",
    "#     'cast-pointer': (0.455, 13.672801),\n",
    "#     'cast-struct': (0.001, 7.118796),\n",
    "#     'definition': (0.0, -37.71391),\n",
    "#     'delegate': (0.0, -3.9902112),\n",
    "#     'memory-access': (0.0, -4.7687745),\n",
    "#     'pointer-arithmetic': (0.0, -17.577158),\n",
    "#     'syscall': (0.0, -22.886244),\n",
    "#     'unused': (0.0, -5.127344)},\n",
    "#    {'atomic': (0.0, -8.450554),\n",
    "#     'efficiency': (0.001, 3.8338575),\n",
    "#     'ffi': (0.0, 0.4648861),\n",
    "#     'generics': (0.994, 10.651982),\n",
    "#     'hide-escape': (0.0, -19.393679),\n",
    "#     'layout': (0.0, -3.1287308),\n",
    "#     'no-gc': (0.0, -17.564428),\n",
    "#     'reflect': (0.0, 1.974027),\n",
    "#     'serialization': (0.0, 0.13683213),\n",
    "#     'types': (0.004, 5.250824),\n",
    "#     'unused': (0.0, -0.42025906)})\n",
    "#   GGNN Preds:\n",
    "#   ({'cast-basic': (0.0, 1.3986396),\n",
    "#     'cast-bytes': (0.993, 19.658085),\n",
    "#     'cast-header': (0.0, -14.408668),\n",
    "#     'cast-pointer': (0.0, 11.910549),\n",
    "#     'cast-struct': (0.0, 6.641031),\n",
    "#     'definition': (0.0, -29.842493),\n",
    "#     'delegate': (0.006, 14.610508),\n",
    "#     'memory-access': (0.0, 5.646147),\n",
    "#     'pointer-arithmetic': (0.0, -32.213),\n",
    "#     'syscall': (0.0, -41.637238),\n",
    "#     'unused': (0.0, -21.783506)},\n",
    "#    {'atomic': (0.0, 10.781407),\n",
    "#     'efficiency': (0.007, 13.999677),\n",
    "#     'ffi': (0.0, -11.787495),\n",
    "#     'generics': (0.0, 11.160701),\n",
    "#     'hide-escape': (0.0, -27.821772),\n",
    "#     'layout': (0.0, -1.9805977),\n",
    "#     'no-gc': (0.0, -40.828663),\n",
    "#     'reflect': (0.0, 3.5758069),\n",
    "#     'serialization': (0.993, 18.984678),\n",
    "#     'types': (0.0, -6.821523),\n",
    "#     'unused': (0.0, -30.649702)})\n",
    "# - 1393 (a4c0182265f30feaa1f2) true: cast-bytes efficiency\n",
    "#   Code: func (m *SortedMap) Get(key string) (value interface{}, ok bool) {\n",
    "#\t          return m.trie.Get(*(*[]byte)(unsafe.Pointer(&key)))\n",
    "#         }\n",
    "\n",
    "# NOTE: On first, third and last two examples serialization was wrongly predicted by GNN (instead of generics/efficiency). Why? DeepSet correctly detects generic, but not efficiency\n",
    "\n",
    "# train_pred = m.predict(train_ds)\n",
    "# train_pred2 = m2.predict(train_ds)\n",
    "# s_pred = m.predict(test_ds)\n",
    "# s_pred2 = m2.predict(test_ds)\n",
    "# s_pred_labels = tf.cast(tf.stack([tf.argmax(s_pred[0], -1), tf.argmax(s_pred[1], -1)], 1), tf.int32)\n",
    "# s_pred_labels2 = tf.cast(tf.stack([tf.argmax(s_pred2[0], -1), tf.argmax(s_pred2[1], -1)], 1), tf.int32)\n",
    "# target_labels = tf.stack(list(test_ds)[0][1], 1)\n",
    "# s_pred\n",
    "# draw_confusion(train_pred, train_slice[1], True)\n",
    "# draw_confusion(train_pred2, train_slice[1], True)\n",
    "# draw_confusion(s_pred, test_slice[1], True)\n",
    "# draw_confusion(s_pred2, test_slice[1], True)\n",
    "#\n",
    "# pred_matches = (s_pred_labels == target_labels).numpy()\n",
    "# pred_matches2 = (s_pred_labels2 == target_labels).numpy()\n",
    "# problem_js = np.where(~(pred_matches[:, 0] | pred_matches[:, 1]))[0]\n",
    "# problem_js2 = np.where(~(pred_matches2[:, 0] | pred_matches2[:, 1]))[0]\n",
    "# set(problem_js) & set(problem_js2)\n",
    "# set(problem_js2) - set(problem_js)\n",
    "# len(problem_js2)\n",
    "#\n",
    "# # j = problem_js[4]\n",
    "# j = 32; i = None\n",
    "# # i = 84\n",
    "# i = debug_graph(i=i, test_j=j, draw=True)\n",
    "# i\n",
    "# preds_to_dicts(s_pred, j)\n",
    "# preds_to_dicts(s_pred2, j)\n",
    "# s_pred_labels2[j]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
