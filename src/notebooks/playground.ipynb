{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import funcy as fy\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "\n",
    "import usgoc.preprocessing.graph.wl2 as wl2\n",
    "import usgoc.datasets.unsafe_go as dataset\n",
    "import usgoc.models.gnn as gnn\n",
    "import usgoc.evaluation.models as em\n",
    "import usgoc.utils as utils\n",
    "import usgoc.metrics.multi as mm\n",
    "import usgoc.evaluation.evaluate as ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 2\n",
    "# mode = \"split_blocks\"\n",
    "mode = \"atomic_blocks\"\n",
    "limit_id = \"v127_d127_f127_p127_core\"\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_tf_fb\"  # \"datatype_flag\" important!\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_tf_fb_ob\"  # \"ob\" important!\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt\"\n",
    "limit_id = \"v0_d127_f0_p0_core\"\n",
    "limit_id = \"v127_d127_f127_p127\"\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_tf_fb_ob_ou_s\"  #\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_fb_ob_ou_s\"  #\n",
    "limit_id = dict(\n",
    "    varname=0, datatype=0, function=0, package=0,\n",
    "    type={\"block\", \"subblock\"},\n",
    "    # type={\"block\", \"subblock\", \"var\"},\n",
    "    blocktype=False, selfref=False, vartype=False,\n",
    "    # datatype_flag={\"Basic\", \"Interface\", \"Named\", \"Pointer\"},\n",
    "    # datatype_flag=True,\n",
    "    datatype_flag=False,\n",
    "    builtin_function=False,\n",
    "    binary_op=False, unary_op=False,\n",
    "    only_core_packages=False\n",
    ")\n",
    "limit_name = dataset.get_limit_name(limit_id)\n",
    "limit_id = \"v127_d127_f127_p127\"\n",
    "batch_size_limit = 200\n",
    "\n",
    "limit_name\n",
    "\n",
    "with utils.cache_env(use_cache=True):\n",
    "  files = dataset.load_filenames()\n",
    "  raw = dataset.load_raw()\n",
    "  ds = dataset.load_dataset(mode=mode)\n",
    "  graphs, targets = ds\n",
    "  full_dims = dataset.create_graph_dims(graphs, mode=mode)\n",
    "\n",
    "# files[683]\n",
    "# files.index(\"/app/raw/unsafe-go-dataset/app/efficiency__cast-pointer/efb20d96d7e6d3b08653.json\")\n",
    "# with utils.cache_env(use_cache=True): gs = dataset.raw_to_graphs(raw, mode=mode)\n",
    "# g = gs[683]; print(g.source_code)\n",
    "# utils.draw_graph(g, layout=\"dot\")\n",
    "# [k for k in g.types_to_pkgs.keys()]\n",
    "# g.types_to_pkgs\n",
    "# h = dataset.collect_node_label_histogram(gs, mode=mode)\n",
    "# [k for k, v in h[\"core_datatype\"].items() if not v]\n",
    "\n",
    "# graphs[0].nodes[4]\n",
    "# utils.draw_graph(graphs[0], layout=\"dot\")\n",
    "\n",
    "# len(dataset.get_dim_limit_dict().keys())\n",
    "\n",
    "# dataset.get_dim_limit_dict()[limit_id]\n",
    "\n",
    "# -%%\n",
    "\n",
    "with utils.cache_env(use_cache=True):\n",
    "  splits = dataset.get_split_idxs(ds)\n",
    "  labels1, labels2 = dataset.create_target_label_dims(ds)\n",
    "  labels1_keys = labels1.keys()\n",
    "  labels2_keys = labels2.keys()\n",
    "  labels1_inv = fy.flip(labels1)\n",
    "  labels2_inv = fy.flip(labels2)\n",
    "\n",
    "# model = gnn.MLP\n",
    "# model = gnn.DeepSets\n",
    "# model = gnn.GCN\n",
    "model = gnn.GIN\n",
    "# model = gnn.GGNN\n",
    "# model = gnn.RGCN\n",
    "# model = gnn.WL2GNN\n",
    "\n",
    "# model1 = em.DeepSetsBuilder\n",
    "# model = em.GGNNBuilder\n",
    "\n",
    "with utils.cache_env(use_cache=True):\n",
    "  dims, train_ds, val_ds, test_ds = dataset.get_encoded_dataset_slices(\n",
    "      ds, model.in_enc, splits, fold, limit_id=limit_id, mode=mode,\n",
    "      batch_size_limit=batch_size_limit)\n",
    "  train_ds = train_ds.cache()\n",
    "  val_ds = val_ds.cache()\n",
    "  train_slice, val_slice, test_slice = dataset.get_dataset_slices(\n",
    "      ds, splits, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_str():\n",
    "  return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "def experiment(model, epochs=150, log=True):\n",
    "  if isinstance(model, kt.HyperModel):\n",
    "    tuner = kt.Hyperband(\n",
    "        model,\n",
    "        objective=\"val_accuracy\",\n",
    "        max_epochs=200, factor=3,\n",
    "        hyperband_iterations=1,\n",
    "        directory=f\"{utils.PROJECT_ROOT}/evaluations\",\n",
    "        project_name=f\"playground_{model.name}\")\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=30)\n",
    "    tuner.search(\n",
    "        train_ds, validation_data=val_ds, verbose=2, epochs=500,\n",
    "        callbacks=[stop_early])\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(\"Best HPs:\", best_hps)\n",
    "    m = tuner.hypermodel.build(best_hps)\n",
    "    patient_stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=100, restore_best_weights=True)\n",
    "    m.fit(\n",
    "        train_ds, validation_data=val_ds, verbose=2, epochs=1000,\n",
    "        callbacks=[patient_stop_early])\n",
    "    res = m.evaluate(test_ds, return_dict=True)\n",
    "    print(res)\n",
    "    return m\n",
    "  else:\n",
    "    m = model(\n",
    "        node_label_count=dims[\"node_label_count\"],\n",
    "        conv_directed=True,\n",
    "        # conv_reverse=False,\n",
    "        conv_layer_units=[400] * 4, fc_layer_units=[200] * 2,\n",
    "        conv_activation=\"tanh\",\n",
    "        conv_inner_activation=\"elu\",\n",
    "        fc_activation=\"tanh\",\n",
    "        pooling=\"min\",\n",
    "        # For 2-WL-GNN:\n",
    "        # conv_activation=\"tanh\",\n",
    "        # conv_inner_activation=\"tanh\",\n",
    "        # fc_activation=\"elu\",\n",
    "        # pooling=\"mean\",\n",
    "        #\n",
    "        # conv_dropout_rate=0.1,\n",
    "        # fc_dropout_rate=0.2,\n",
    "        conv_batch_norm=True,\n",
    "        fc_batch_norm=True,\n",
    "        out_activation=None,\n",
    "        learning_rate=0.001)\n",
    "\n",
    "    tb = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=f\"/app/logs/{time_str()}_{model.name}_{limit_name}_fold{fold}\",\n",
    "        histogram_freq=10,\n",
    "        embeddings_freq=10,\n",
    "        write_graph=True,\n",
    "        update_freq=\"batch\")\n",
    "    patient_stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=100, restore_best_weights=True)\n",
    "    m.fit(\n",
    "        train_ds, validation_data=val_ds, verbose=2, epochs=epochs,\n",
    "        callbacks=[tb, patient_stop_early] if log else [patient_stop_early])\n",
    "    res = m.evaluate(test_ds, return_dict=True)\n",
    "    print(res)\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm._groups = dict()\n",
    "# m = experiment(model1)\n",
    "# m.save(f\"{utils.PROJECT_ROOT}/logs/test\")\n",
    "# m2 = tf.keras.models.load_model(f\"{utils.PROJECT_ROOT}/logs/test\", custom_objects=dict(SparseMultiAccuracy=mm.SparseMultiAccuracy))\n",
    "\n",
    "m = ee.evaluate(model.name, convert_mode=mode,\n",
    "                limit_id=limit_id, fold=fold, repeat=0, dry=True,\n",
    "                return_models=True)\n",
    "# m[0][2].evaluate(test_ds, return_dict=True)\n",
    "# m = experiment(model1)\n",
    "\n",
    "# m2 = experiment(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(m, show_layer_activations=True, show_dtype=True)\n",
    "# m2.predict(test_ds)[-1][\"X\"]\n",
    "[(l.name, l.losses) for l in m.layers]\n",
    "\n",
    "def add_conv_output_to_model(model):\n",
    "  pooling_layer = fy.first(fy.filter(lambda l: \"pooling\" in l.name, model.layers))\n",
    "  return tf.keras.Model(m.input, outputs=model.output + (pooling_layer.input,))\n",
    "\n",
    "m2 = add_conv_output_to_model(m)\n",
    "\n",
    "def gradcam(model, input):\n",
    "  X = input[\"X\"]\n",
    "  graph_X = input[\"graph_X\"]\n",
    "  class_counts = (\n",
    "    model.output[0].shape[-1], \n",
    "    model.output[1].shape[-1])\n",
    "  vertex_importance = ([], [])\n",
    "  vertex_feature_importance = ([], [])\n",
    "  graph_feature_importance = ([], [])\n",
    "  \n",
    "  with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(X)\n",
    "    g.watch(graph_X)\n",
    "    output = model(input)\n",
    "    conv_X = output[-1][\"X\"]\n",
    "\n",
    "    for label_idx in [0, 1]:\n",
    "      for c in range(class_counts[label_idx]):\n",
    "        pred = output[label_idx][:, c]\n",
    "        with g.stop_recording():\n",
    "          X_grad = g.gradient(pred, conv_X)\n",
    "          vertex_importance[label_idx].append(tf.nn.relu(\n",
    "            tf.math.reduce_sum(X_grad * conv_X, axis=-1)))\n",
    "          X_grad, graph_grad = g.gradient(pred, (X, graph_X))\n",
    "          vertex_feature_importance[label_idx].append(tf.nn.relu(\n",
    "            tf.math.unsorted_segment_sum(\n",
    "              X_grad * X, input[\"graph_idx\"], input[\"n\"].shape[0])))\n",
    "          graph_feature_importance[label_idx].append(\n",
    "            tf.nn.relu(graph_grad * graph_X))\n",
    "        \n",
    "  return tuple((\n",
    "    tf.stack(vertex_importance[i], axis=0),\n",
    "    tf.stack(vertex_feature_importance[i], axis=0),\n",
    "    tf.stack(graph_feature_importance[i], axis=0)\n",
    "  ) for i in [0, 1])\n",
    "\n",
    "for input, targets in test_ds:\n",
    "  target1, target2 = targets\n",
    "  n = input[\"n\"]\n",
    "  marked_idx = input[\"marked_idx\"]\n",
    "  (l1_vimp, l1_fimp, l1_gimp), (l2_vimp, l2_fimp, l2_gimp) = gradcam(m2, input)\n",
    "  print(marked_idx[0], target1[0])\n",
    "  print(l1_vimp[:, :n[0]])\n",
    "  print(l1_fimp[:, 0], l1_gimp[:, 0])\n",
    "  break\n",
    "\n",
    "m2.output[0].shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_graph(i=None, file=None, test_j=None, val_j=None, draw=True):\n",
    "  if i is None:\n",
    "    if file is not None:\n",
    "      i = fy.first(fy.filter(\n",
    "          lambda e: file in e[1], enumerate(files)[0]))\n",
    "    elif test_j is not None:\n",
    "      i = splits[fold][\"test\"][test_j]\n",
    "    elif val_j is not None:\n",
    "      i = splits[fold][\"validation\"][val_j]\n",
    "  print(graphs[i].source_code)\n",
    "  print(files[i], labels1_inv[targets[0][i]], labels2_inv[targets[1][i]])\n",
    "  if draw:\n",
    "    utils.draw_graph(graphs[i], edge_colors=True, layout=\"dot\")\n",
    "  return i\n",
    "\n",
    "\n",
    "def get_singleton_ds(i):\n",
    "  with utils.cache_env(use_cache=False):\n",
    "    return dataset.dataset_encoders[model.in_enc](dataset.slice(ds, [i]), dims)\n",
    "\n",
    "\n",
    "def preds_to_dicts(preds, j):\n",
    "  prob1 = np.around(tf.nn.softmax(preds[0], -1).numpy()[j], 3)\n",
    "  prob2 = np.around(tf.nn.softmax(preds[1], -1).numpy()[j], 3)\n",
    "  d1 = fy.zipdict(labels1_keys, zip(list(prob1), list(preds[0][j])))\n",
    "  d2 = fy.zipdict(labels2_keys, zip(list(prob2), list(preds[1][j])))\n",
    "  return d1, d2\n",
    "\n",
    "\n",
    "def draw_confusion(pred_labels, target_labels, normalize=True):\n",
    "  m1 = mm.sparse_multi_confusion_matrix(\n",
    "      tf.constant(target_labels[0], dtype=tf.int32),\n",
    "      tf.constant(pred_labels[0], dtype=tf.int32)).numpy()\n",
    "  m2 = mm.sparse_multi_confusion_matrix(\n",
    "      tf.constant(target_labels[1], dtype=tf.int32),\n",
    "      tf.constant(pred_labels[1], dtype=tf.int32)).numpy()\n",
    "  a1 = np.sum(np.diag(m1)) / np.sum(m1)\n",
    "  a2 = np.sum(np.diag(m2)) / np.sum(m2)\n",
    "  print(\"L1-Acc:\", a1, \"L2-Acc:\", a2)\n",
    "  if normalize:\n",
    "    m1 = np.around(m1 / np.sum(m1, axis=1, keepdims=True), 2) * 100\n",
    "    m2 = np.around(m2 / np.sum(m2, axis=1, keepdims=True), 2) * 100\n",
    "  utils.draw_confusion_matrix(m1.astype(int), labels1_keys)\n",
    "  utils.draw_confusion_matrix(m2.astype(int), labels2_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_graph(file=\"5179906774f18e1f8520\", draw=False)\n",
    "# debug_graph(401)\n",
    "\n",
    "\n",
    "# interesting i's: 40, 70, 874 (44b41ab329d2624a449e),\n",
    "# Instances on which both DeepSets and GGNN fail (both labels):\n",
    "# - 43 (a793d8b779605120ec52) true: cast-basic ffi (short snippet)\n",
    "# - 79 (836d21cfe55279cd8e46) true: delegate atomic (long snippet)\n",
    "# - 1236 (27b69b7c042b73cf9275) true: memory-access layout (short/medium snippet)\n",
    "\n",
    "# Instances on wihich only GGNN fails (but not DeepSets):\n",
    "# - 84 (b01a05c45ecc95b792c8) true: cast-header types (short/medium)\n",
    "#   Code:\n",
    "#   func bytesHash(b []byte, seed uintptr) uintptr {\n",
    "#\t    s := (*slice)(unsafe.Pointer(&b))\n",
    "#\t    return memhash(s.array, seed, uintptr(s.len))\n",
    "#   }\n",
    "# - 1015 (39edbcf5d9443fc73e3a) true: pointer-arithmetic serialization (short)\n",
    "# - 1313 (b54f66ea4c799f26617d) true: cast-struct generics (short)\n",
    "#   Code: func (h *Header) Int8s() []int8 { return *(*[]int8)(unsafe.Pointer(h)) }\n",
    "#   DeepSets Preds:\n",
    "#   ({'cast-basic': (0.544, 13.852097),\n",
    "#     'cast-bytes': (0.0, -0.26986185),\n",
    "#     'cast-header': (0.0, -3.9276118),\n",
    "#     'cast-pointer': (0.455, 13.672801),\n",
    "#     'cast-struct': (0.001, 7.118796),\n",
    "#     'definition': (0.0, -37.71391),\n",
    "#     'delegate': (0.0, -3.9902112),\n",
    "#     'memory-access': (0.0, -4.7687745),\n",
    "#     'pointer-arithmetic': (0.0, -17.577158),\n",
    "#     'syscall': (0.0, -22.886244),\n",
    "#     'unused': (0.0, -5.127344)},\n",
    "#    {'atomic': (0.0, -8.450554),\n",
    "#     'efficiency': (0.001, 3.8338575),\n",
    "#     'ffi': (0.0, 0.4648861),\n",
    "#     'generics': (0.994, 10.651982),\n",
    "#     'hide-escape': (0.0, -19.393679),\n",
    "#     'layout': (0.0, -3.1287308),\n",
    "#     'no-gc': (0.0, -17.564428),\n",
    "#     'reflect': (0.0, 1.974027),\n",
    "#     'serialization': (0.0, 0.13683213),\n",
    "#     'types': (0.004, 5.250824),\n",
    "#     'unused': (0.0, -0.42025906)})\n",
    "#   GGNN Preds:\n",
    "#   ({'cast-basic': (0.0, 1.3986396),\n",
    "#     'cast-bytes': (0.993, 19.658085),\n",
    "#     'cast-header': (0.0, -14.408668),\n",
    "#     'cast-pointer': (0.0, 11.910549),\n",
    "#     'cast-struct': (0.0, 6.641031),\n",
    "#     'definition': (0.0, -29.842493),\n",
    "#     'delegate': (0.006, 14.610508),\n",
    "#     'memory-access': (0.0, 5.646147),\n",
    "#     'pointer-arithmetic': (0.0, -32.213),\n",
    "#     'syscall': (0.0, -41.637238),\n",
    "#     'unused': (0.0, -21.783506)},\n",
    "#    {'atomic': (0.0, 10.781407),\n",
    "#     'efficiency': (0.007, 13.999677),\n",
    "#     'ffi': (0.0, -11.787495),\n",
    "#     'generics': (0.0, 11.160701),\n",
    "#     'hide-escape': (0.0, -27.821772),\n",
    "#     'layout': (0.0, -1.9805977),\n",
    "#     'no-gc': (0.0, -40.828663),\n",
    "#     'reflect': (0.0, 3.5758069),\n",
    "#     'serialization': (0.993, 18.984678),\n",
    "#     'types': (0.0, -6.821523),\n",
    "#     'unused': (0.0, -30.649702)})\n",
    "# - 1393 (a4c0182265f30feaa1f2) true: cast-bytes efficiency\n",
    "#   Code: func (m *SortedMap) Get(key string) (value interface{}, ok bool) {\n",
    "#\t          return m.trie.Get(*(*[]byte)(unsafe.Pointer(&key)))\n",
    "#         }\n",
    "\n",
    "# NOTE: On first, third and last two examples serialization was wrongly predicted by GNN (instead of generics/efficiency). Why? DeepSet correctly detects generic, but not efficiency\n",
    "\n",
    "# train_pred = m.predict(train_ds)\n",
    "# train_pred2 = m2.predict(train_ds)\n",
    "# s_pred = m.predict(test_ds)\n",
    "# s_pred2 = m2.predict(test_ds)\n",
    "# s_pred_labels = tf.cast(tf.stack([tf.argmax(s_pred[0], -1), tf.argmax(s_pred[1], -1)], 1), tf.int32)\n",
    "# s_pred_labels2 = tf.cast(tf.stack([tf.argmax(s_pred2[0], -1), tf.argmax(s_pred2[1], -1)], 1), tf.int32)\n",
    "# target_labels = tf.stack(list(test_ds)[0][1], 1)\n",
    "# s_pred\n",
    "# draw_confusion(train_pred, train_slice[1], True)\n",
    "# draw_confusion(train_pred2, train_slice[1], True)\n",
    "# draw_confusion(s_pred, test_slice[1], True)\n",
    "# draw_confusion(s_pred2, test_slice[1], True)\n",
    "#\n",
    "# pred_matches = (s_pred_labels == target_labels).numpy()\n",
    "# pred_matches2 = (s_pred_labels2 == target_labels).numpy()\n",
    "# problem_js = np.where(~(pred_matches[:, 0] | pred_matches[:, 1]))[0]\n",
    "# problem_js2 = np.where(~(pred_matches2[:, 0] | pred_matches2[:, 1]))[0]\n",
    "# set(problem_js) & set(problem_js2)\n",
    "# set(problem_js2) - set(problem_js)\n",
    "# len(problem_js2)\n",
    "#\n",
    "# # j = problem_js[4]\n",
    "# j = 32; i = None\n",
    "# # i = 84\n",
    "# i = debug_graph(i=i, test_j=j, draw=True)\n",
    "# i\n",
    "# preds_to_dicts(s_pred, j)\n",
    "# preds_to_dicts(s_pred2, j)\n",
    "# s_pred_labels2[j]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
