{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import funcy as fy\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "\n",
    "import usgoc.preprocessing.graph.wl2 as wl2\n",
    "import usgoc.datasets.unsafe_go as dataset\n",
    "import usgoc.models.gnn as gnn\n",
    "import usgoc.evaluation.models as em\n",
    "import usgoc.utils as utils\n",
    "import usgoc.metrics.multi as mm\n",
    "import usgoc.evaluation.evaluate as ee\n",
    "import usgoc.postprocessing.explain as explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 2\n",
    "# mode = \"split_blocks\"\n",
    "mode = \"atomic_blocks\"\n",
    "limit_id = \"v127_d127_f127_p127_core\"\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_tf_fb\"  # \"datatype_flag\" important!\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_tf_fb_ob\"  # \"ob\" important!\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt\"\n",
    "limit_id = \"v0_d127_f0_p0_core\"\n",
    "limit_id = \"v127_d127_f127_p127\"\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_tf_fb_ob_ou_s\"  #\n",
    "limit_id = \"v0_d0_f0_p0_no_v_vt_bt_fb_ob_ou_s\"  #\n",
    "limit_id = dict(\n",
    "    varname=0, datatype=0, function=0, package=0,\n",
    "    type={\"block\", \"subblock\"},\n",
    "    # type={\"block\", \"subblock\", \"var\"},\n",
    "    blocktype=False, selfref=False, vartype=False,\n",
    "    # datatype_flag={\"Basic\", \"Interface\", \"Named\", \"Pointer\"},\n",
    "    # datatype_flag=True,\n",
    "    datatype_flag=False,\n",
    "    builtin_function=False,\n",
    "    binary_op=False, unary_op=False,\n",
    "    only_core_packages=False\n",
    ")\n",
    "limit_name = dataset.get_limit_name(limit_id)\n",
    "limit_id = \"v127_d127_f127_p127\"\n",
    "batch_size_limit = 200\n",
    "\n",
    "limit_name\n",
    "\n",
    "with utils.cache_env(use_cache=True):\n",
    "  files = dataset.load_filenames()\n",
    "  raw = dataset.load_raw()\n",
    "  ds = dataset.load_dataset(mode=mode)\n",
    "  graphs, targets = ds\n",
    "  full_dims = dataset.create_graph_dims(graphs, mode=mode)\n",
    "\n",
    "# files[683]\n",
    "# files.index(\"/app/raw/unsafe-go-dataset/app/efficiency__cast-pointer/efb20d96d7e6d3b08653.json\")\n",
    "# with utils.cache_env(use_cache=True): gs = dataset.raw_to_graphs(raw, mode=mode)\n",
    "# g = gs[683]; print(g.source_code)\n",
    "# utils.draw_graph(g, layout=\"dot\")\n",
    "# [k for k in g.types_to_pkgs.keys()]\n",
    "# g.types_to_pkgs\n",
    "# h = dataset.collect_node_label_histogram(gs, mode=mode)\n",
    "# [k for k, v in h[\"core_datatype\"].items() if not v]\n",
    "\n",
    "# graphs[0].nodes[4]\n",
    "# utils.draw_graph(graphs[0], layout=\"dot\")\n",
    "\n",
    "# len(dataset.get_dim_limit_dict().keys())\n",
    "\n",
    "# dataset.get_dim_limit_dict()[limit_id]\n",
    "\n",
    "# -%%\n",
    "\n",
    "with utils.cache_env(use_cache=True):\n",
    "  splits = dataset.get_split_idxs(ds)\n",
    "  labels1, labels2 = dataset.create_target_label_dims(ds)\n",
    "  labels1_keys = labels1.keys()\n",
    "  labels2_keys = labels2.keys()\n",
    "  labels1_inv = fy.flip(labels1)\n",
    "  labels2_inv = fy.flip(labels2)\n",
    "\n",
    "# model = gnn.MLP\n",
    "model = gnn.DeepSets\n",
    "# model = gnn.GCN\n",
    "# model = gnn.GIN\n",
    "# model = gnn.GGNN\n",
    "# model = gnn.RGCN\n",
    "# model = gnn.WL2GNN\n",
    "\n",
    "# model1 = em.DeepSetsBuilder\n",
    "# model = em.GGNNBuilder\n",
    "\n",
    "# with utils.cache_env(use_cache=True):\n",
    "#   dims, train_ds, val_ds, test_ds = dataset.get_encoded_dataset_slices(\n",
    "#       ds, model.in_enc, splits, fold, limit_id=limit_id, mode=mode,\n",
    "#       batch_size_limit=batch_size_limit)\n",
    "#   train_ds = train_ds.cache()\n",
    "#   val_ds = val_ds.cache()\n",
    "#   train_slice, val_slice, test_slice = dataset.get_dataset_slices(\n",
    "#       ds, splits, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_str():\n",
    "  return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "def experiment(model, epochs=150, log=True):\n",
    "  if isinstance(model, kt.HyperModel):\n",
    "    tuner = kt.Hyperband(\n",
    "        model,\n",
    "        objective=\"val_accuracy\",\n",
    "        max_epochs=200, factor=3,\n",
    "        hyperband_iterations=1,\n",
    "        directory=f\"{utils.PROJECT_ROOT}/evaluations\",\n",
    "        project_name=f\"playground_{model.name}\")\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=30)\n",
    "    tuner.search(\n",
    "        train_ds, validation_data=val_ds, verbose=2, epochs=500,\n",
    "        callbacks=[stop_early])\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(\"Best HPs:\", best_hps)\n",
    "    m = tuner.hypermodel.build(best_hps)\n",
    "    patient_stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=100, restore_best_weights=True)\n",
    "    m.fit(\n",
    "        train_ds, validation_data=val_ds, verbose=2, epochs=1000,\n",
    "        callbacks=[patient_stop_early])\n",
    "    res = m.evaluate(test_ds, return_dict=True)\n",
    "    print(res)\n",
    "    return m\n",
    "  else:\n",
    "    m = model(\n",
    "        node_label_count=dims[\"node_label_count\"],\n",
    "        conv_directed=True,\n",
    "        # conv_reverse=False,\n",
    "        conv_layer_units=[400] * 4, fc_layer_units=[200] * 2,\n",
    "        conv_activation=\"tanh\",\n",
    "        conv_inner_activation=\"elu\",\n",
    "        fc_activation=\"tanh\",\n",
    "        pooling=\"min\",\n",
    "        # For 2-WL-GNN:\n",
    "        # conv_activation=\"tanh\",\n",
    "        # conv_inner_activation=\"tanh\",\n",
    "        # fc_activation=\"elu\",\n",
    "        # pooling=\"mean\",\n",
    "        #\n",
    "        # conv_dropout_rate=0.1,\n",
    "        # fc_dropout_rate=0.2,\n",
    "        conv_batch_norm=True,\n",
    "        fc_batch_norm=True,\n",
    "        out_activation=None,\n",
    "        learning_rate=0.001)\n",
    "\n",
    "    tb = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=f\"/app/logs/{time_str()}_{model.name}_{limit_name}_fold{fold}\",\n",
    "        histogram_freq=10,\n",
    "        embeddings_freq=10,\n",
    "        write_graph=True,\n",
    "        update_freq=\"batch\")\n",
    "    patient_stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=100, restore_best_weights=True)\n",
    "    m.fit(\n",
    "        train_ds, validation_data=val_ds, verbose=2, epochs=epochs,\n",
    "        callbacks=[tb, patient_stop_early] if log else [patient_stop_early])\n",
    "    res = m.evaluate(test_ds, return_dict=True)\n",
    "    print(res)\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm._groups = dict()\n",
    "# m = experiment(model1)\n",
    "# m.save(f\"{utils.PROJECT_ROOT}/logs/test\")\n",
    "# m2 = tf.keras.models.load_model(f\"{utils.PROJECT_ROOT}/logs/test\", custom_objects=dict(SparseMultiAccuracy=mm.SparseMultiAccuracy))\n",
    "\n",
    "(m0, dims0, train0, val0, test0), (m1, dims1, train1, val1, test1)  = ee.evaluate(model.name, convert_mode=mode,\n",
    "  limit_id=limit_id, folds=2, repeat=0, dry=True,\n",
    "  return_models=True, return_ds=True)\n",
    "# m[0][2].evaluate(test_ds, return_dict=True)\n",
    "# m = experiment(model1)\n",
    "\n",
    "# m2 = experiment(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_names0 = dataset.dims_to_labels(dims0, model.in_enc)\n",
    "dim_names1 = dataset.dims_to_labels(dims1, model.in_enc)\n",
    "\n",
    "def merge_dims(dims_list):\n",
    "  merged_node_dims = dict()\n",
    "  remap_idxs = []\n",
    "  new_idx = 0\n",
    "  for dims in dims_list:\n",
    "    remap_idx = np.empty(dims[\"node_label_count\"], dtype=np.int32)\n",
    "    for label_type, sublabels in dims[\"node_labels\"].items():\n",
    "      if label_type not in merged_node_dims:\n",
    "        merged_sublabels = dict()\n",
    "        merged_node_dims[label_type] = merged_sublabels\n",
    "      else:\n",
    "        merged_sublabels = merged_node_dims[label_type]\n",
    "      for sublabel, idx in sublabels.items():\n",
    "        if sublabel not in merged_sublabels:\n",
    "          merged_sublabels[sublabel] = new_idx\n",
    "          remap_idx[idx] = new_idx\n",
    "          new_idx += 1\n",
    "        else:\n",
    "          remap_idx[idx] = merged_sublabels[sublabel]\n",
    "    remap_idxs.append(remap_idx)\n",
    "  dims0 = dims_list[0]\n",
    "\n",
    "  return dict(\n",
    "    node_labels=merged_node_dims,\n",
    "    node_label_count=new_idx,\n",
    "    edge_labels=dims0[\"edge_labels\"],\n",
    "    edge_label_count=dims0[\"edge_label_count\"]\n",
    "  ), remap_idxs\n",
    "\n",
    "def apply_remap_idx(\n",
    "  X, remap_idx, node_label_count, in_enc, with_marked_idx=True):\n",
    "  offset = 0\n",
    "  if with_marked_idx:\n",
    "    offset += 1\n",
    "  if in_enc == \"wl2\":\n",
    "    offset += 3\n",
    "  old_node_label_count = remap_idx.size\n",
    "  old_final_offset = offset + old_node_label_count\n",
    "  final_offset = offset + node_label_count\n",
    "  node_label_diff = node_label_count - old_node_label_count\n",
    "  X_remapped = np.zeros(X.shape[:-1] + (X.shape[-1] + node_label_diff,), dtype=X.dtype)\n",
    "  X_remapped[..., :offset] = X[..., :offset]\n",
    "  X_remapped[..., remap_idx + offset] = X[..., offset:old_final_offset]\n",
    "  X_remapped[..., final_offset:] = X[..., old_final_offset:]\n",
    "  return X_remapped\n",
    "\n",
    "def apply_remap_idxs(Xs, remap_idxs, node_label_count, in_enc, with_marked_idx=True):\n",
    "  return [\n",
    "    apply_remap_idx(X, remap_idx, node_label_count, in_enc, with_marked_idx)\n",
    "    for X, remap_idx in zip(Xs, remap_idxs)]\n",
    "\n",
    "dims_merged, remap_idxs = merge_dims([dims0, dims1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_fimps0, l2_fimps0 = explain.compute_importances(m0, test0)\n",
    "l1_fimps1, l2_fimps1 = explain.compute_importances(m1, test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_fimps0.shape, l1_fimps1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_fimps0_remap, l1_fimps1_remap = apply_remap_idxs(\n",
    "    [l1_fimps0, l1_fimps1], remap_idxs, dims_merged[\"node_label_count\"], model.in_enc)\n",
    "l2_fimps0_remap, l2_fimps1_remap = apply_remap_idxs(\n",
    "    [l2_fimps0, l2_fimps1], remap_idxs, dims_merged[\"node_label_count\"], model.in_enc)\n",
    "l1_fimps = l1_fimps0_remap + l1_fimps1_remap\n",
    "l2_fimps = l2_fimps0_remap + l2_fimps1_remap\n",
    "\n",
    "# utils.draw_feature_importance_chart([\n",
    "#   (\"Label 1\", list(labels1_keys) + [\"Combined\"], explain.group_feature_importance(l1_fimps0)),\n",
    "#   (\"Label 2\", list(labels2_keys) + [\"Combined\"], explain.group_feature_importance(l2_fimps0))\n",
    "# ], np.array(dataset.dims_to_labels(dims0, model.in_enc)))\n",
    "# utils.draw_feature_importance_chart([\n",
    "#   (\"Label 1\", list(labels1_keys) + [\"Combined\"], explain.group_feature_importance(l1_fimps1)),\n",
    "#   (\"Label 2\", list(labels2_keys) + [\"Combined\"], explain.group_feature_importance(l2_fimps1))\n",
    "# ], np.array(dataset.dims_to_labels(dims1, model.in_enc)))\n",
    "utils.draw_feature_importance_chart([\n",
    "  (\"Label 1\", list(labels1_keys) + [\"Combined\"], explain.group_feature_importance(l1_fimps)),\n",
    "  (\"Label 2\", list(labels2_keys) + [\"Combined\"], explain.group_feature_importance(l2_fimps))\n",
    "], np.array(dataset.dims_to_labels(dims_merged, model.in_enc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_graph(i=None, file=None, test_j=None, val_j=None, draw=True):\n",
    "  if i is None:\n",
    "    if file is not None:\n",
    "      i = fy.first(fy.filter(\n",
    "          lambda e: file in e[1], enumerate(files)[0]))\n",
    "    elif test_j is not None:\n",
    "      i = splits[fold][\"test\"][test_j]\n",
    "    elif val_j is not None:\n",
    "      i = splits[fold][\"validation\"][val_j]\n",
    "  print(graphs[i].source_code)\n",
    "  print(files[i], labels1_inv[targets[0][i]], labels2_inv[targets[1][i]])\n",
    "  if draw:\n",
    "    utils.draw_graph(graphs[i], edge_colors=True, layout=\"dot\")\n",
    "  return i\n",
    "\n",
    "\n",
    "def get_singleton_ds(i):\n",
    "  with utils.cache_env(use_cache=False):\n",
    "    return dataset.dataset_encoders[model.in_enc](dataset.slice(ds, [i]), dims)\n",
    "\n",
    "\n",
    "def preds_to_dicts(preds, j):\n",
    "  prob1 = np.around(tf.nn.softmax(preds[0], -1).numpy()[j], 3)\n",
    "  prob2 = np.around(tf.nn.softmax(preds[1], -1).numpy()[j], 3)\n",
    "  d1 = fy.zipdict(labels1_keys, zip(list(prob1), list(preds[0][j])))\n",
    "  d2 = fy.zipdict(labels2_keys, zip(list(prob2), list(preds[1][j])))\n",
    "  return d1, d2\n",
    "\n",
    "\n",
    "def draw_confusion(pred_labels, target_labels, normalize=True):\n",
    "  m1 = mm.sparse_multi_confusion_matrix(\n",
    "      tf.constant(target_labels[0], dtype=tf.int32),\n",
    "      tf.constant(pred_labels[0], dtype=tf.int32)).numpy()\n",
    "  m2 = mm.sparse_multi_confusion_matrix(\n",
    "      tf.constant(target_labels[1], dtype=tf.int32),\n",
    "      tf.constant(pred_labels[1], dtype=tf.int32)).numpy()\n",
    "  a1 = np.sum(np.diag(m1)) / np.sum(m1)\n",
    "  a2 = np.sum(np.diag(m2)) / np.sum(m2)\n",
    "  print(\"L1-Acc:\", a1, \"L2-Acc:\", a2)\n",
    "  if normalize:\n",
    "    m1 = np.around(m1 / np.sum(m1, axis=1, keepdims=True), 2) * 100\n",
    "    m2 = np.around(m2 / np.sum(m2, axis=1, keepdims=True), 2) * 100\n",
    "  utils.draw_confusion_matrix(m1.astype(int), labels1_keys)\n",
    "  utils.draw_confusion_matrix(m2.astype(int), labels2_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_graph(file=\"5179906774f18e1f8520\", draw=False)\n",
    "# debug_graph(401)\n",
    "\n",
    "\n",
    "# interesting i's: 40, 70, 874 (44b41ab329d2624a449e),\n",
    "# Instances on which both DeepSets and GGNN fail (both labels):\n",
    "# - 43 (a793d8b779605120ec52) true: cast-basic ffi (short snippet)\n",
    "# - 79 (836d21cfe55279cd8e46) true: delegate atomic (long snippet)\n",
    "# - 1236 (27b69b7c042b73cf9275) true: memory-access layout (short/medium snippet)\n",
    "\n",
    "# Instances on wihich only GGNN fails (but not DeepSets):\n",
    "# - 84 (b01a05c45ecc95b792c8) true: cast-header types (short/medium)\n",
    "#   Code:\n",
    "#   func bytesHash(b []byte, seed uintptr) uintptr {\n",
    "#\t    s := (*slice)(unsafe.Pointer(&b))\n",
    "#\t    return memhash(s.array, seed, uintptr(s.len))\n",
    "#   }\n",
    "# - 1015 (39edbcf5d9443fc73e3a) true: pointer-arithmetic serialization (short)\n",
    "# - 1313 (b54f66ea4c799f26617d) true: cast-struct generics (short)\n",
    "#   Code: func (h *Header) Int8s() []int8 { return *(*[]int8)(unsafe.Pointer(h)) }\n",
    "#   DeepSets Preds:\n",
    "#   ({'cast-basic': (0.544, 13.852097),\n",
    "#     'cast-bytes': (0.0, -0.26986185),\n",
    "#     'cast-header': (0.0, -3.9276118),\n",
    "#     'cast-pointer': (0.455, 13.672801),\n",
    "#     'cast-struct': (0.001, 7.118796),\n",
    "#     'definition': (0.0, -37.71391),\n",
    "#     'delegate': (0.0, -3.9902112),\n",
    "#     'memory-access': (0.0, -4.7687745),\n",
    "#     'pointer-arithmetic': (0.0, -17.577158),\n",
    "#     'syscall': (0.0, -22.886244),\n",
    "#     'unused': (0.0, -5.127344)},\n",
    "#    {'atomic': (0.0, -8.450554),\n",
    "#     'efficiency': (0.001, 3.8338575),\n",
    "#     'ffi': (0.0, 0.4648861),\n",
    "#     'generics': (0.994, 10.651982),\n",
    "#     'hide-escape': (0.0, -19.393679),\n",
    "#     'layout': (0.0, -3.1287308),\n",
    "#     'no-gc': (0.0, -17.564428),\n",
    "#     'reflect': (0.0, 1.974027),\n",
    "#     'serialization': (0.0, 0.13683213),\n",
    "#     'types': (0.004, 5.250824),\n",
    "#     'unused': (0.0, -0.42025906)})\n",
    "#   GGNN Preds:\n",
    "#   ({'cast-basic': (0.0, 1.3986396),\n",
    "#     'cast-bytes': (0.993, 19.658085),\n",
    "#     'cast-header': (0.0, -14.408668),\n",
    "#     'cast-pointer': (0.0, 11.910549),\n",
    "#     'cast-struct': (0.0, 6.641031),\n",
    "#     'definition': (0.0, -29.842493),\n",
    "#     'delegate': (0.006, 14.610508),\n",
    "#     'memory-access': (0.0, 5.646147),\n",
    "#     'pointer-arithmetic': (0.0, -32.213),\n",
    "#     'syscall': (0.0, -41.637238),\n",
    "#     'unused': (0.0, -21.783506)},\n",
    "#    {'atomic': (0.0, 10.781407),\n",
    "#     'efficiency': (0.007, 13.999677),\n",
    "#     'ffi': (0.0, -11.787495),\n",
    "#     'generics': (0.0, 11.160701),\n",
    "#     'hide-escape': (0.0, -27.821772),\n",
    "#     'layout': (0.0, -1.9805977),\n",
    "#     'no-gc': (0.0, -40.828663),\n",
    "#     'reflect': (0.0, 3.5758069),\n",
    "#     'serialization': (0.993, 18.984678),\n",
    "#     'types': (0.0, -6.821523),\n",
    "#     'unused': (0.0, -30.649702)})\n",
    "# - 1393 (a4c0182265f30feaa1f2) true: cast-bytes efficiency\n",
    "#   Code: func (m *SortedMap) Get(key string) (value interface{}, ok bool) {\n",
    "#\t          return m.trie.Get(*(*[]byte)(unsafe.Pointer(&key)))\n",
    "#         }\n",
    "\n",
    "# NOTE: On first, third and last two examples serialization was wrongly predicted by GNN (instead of generics/efficiency). Why? DeepSet correctly detects generic, but not efficiency\n",
    "\n",
    "# train_pred = m.predict(train_ds)\n",
    "# train_pred2 = m2.predict(train_ds)\n",
    "# s_pred = m.predict(test_ds)\n",
    "# s_pred2 = m2.predict(test_ds)\n",
    "# s_pred_labels = tf.cast(tf.stack([tf.argmax(s_pred[0], -1), tf.argmax(s_pred[1], -1)], 1), tf.int32)\n",
    "# s_pred_labels2 = tf.cast(tf.stack([tf.argmax(s_pred2[0], -1), tf.argmax(s_pred2[1], -1)], 1), tf.int32)\n",
    "# target_labels = tf.stack(list(test_ds)[0][1], 1)\n",
    "# s_pred\n",
    "# draw_confusion(train_pred, train_slice[1], True)\n",
    "# draw_confusion(train_pred2, train_slice[1], True)\n",
    "# draw_confusion(s_pred, test_slice[1], True)\n",
    "# draw_confusion(s_pred2, test_slice[1], True)\n",
    "#\n",
    "# pred_matches = (s_pred_labels == target_labels).numpy()\n",
    "# pred_matches2 = (s_pred_labels2 == target_labels).numpy()\n",
    "# problem_js = np.where(~(pred_matches[:, 0] | pred_matches[:, 1]))[0]\n",
    "# problem_js2 = np.where(~(pred_matches2[:, 0] | pred_matches2[:, 1]))[0]\n",
    "# set(problem_js) & set(problem_js2)\n",
    "# set(problem_js2) - set(problem_js)\n",
    "# len(problem_js2)\n",
    "#\n",
    "# # j = problem_js[4]\n",
    "# j = 32; i = None\n",
    "# # i = 84\n",
    "# i = debug_graph(i=i, test_j=j, draw=True)\n",
    "# i\n",
    "# preds_to_dicts(s_pred, j)\n",
    "# preds_to_dicts(s_pred2, j)\n",
    "# s_pred_labels2[j]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
